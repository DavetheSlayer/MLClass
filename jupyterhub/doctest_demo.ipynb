{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "30295c5bec572e859485b1ffa5e89b8b3e2022ef6e3e739c1ac40f143a557caf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doctest in Jupyter Notebook\n",
    "\n",
    "## Step 1 \n",
    "First, import the doctest module to run test cases in docstring \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Then, define you own functions. Be sure that the test cases are included, e.g.,  copying them completely from programming template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a,b):\n",
    "    \"\"\"\n",
    "\n",
    "    >>> f(1,4)\n",
    "    5\n",
    "    >>> f(3.4, 5.6)\n",
    "    9.0\n",
    "    \"\"\"\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Lastly, test your function. Just change `f` below to whatever function you wanna test. \n",
    "\n",
    "### If you see nothing, \n",
    "it means that all test cases have been passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doctest.run_docstring_examples(f, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you see something, \n",
    "then the discrepancy between the expected output and your output is printed, like the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "**********************************************************************\nFile &quot;__main__&quot;, line 6, in NoName\nFailed example:\n    g(10,  0.0)\nExpected:\n    10\nGot:\n    10.0\n"
    }
   ],
   "source": [
    "def g(a, b):\n",
    "    \"\"\"\n",
    "\n",
    "    >>> g(1,2)\n",
    "    -1\n",
    "    >>> g(10,  0.0)\n",
    "    10\n",
    "    \"\"\"\n",
    "    return a-b\n",
    "doctest.run_docstring_examples(g, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of course, you can also test all functions at once\n",
    "Again, only errors will raise. If you see nothing, you are all good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "**********************************************************************\nFile &quot;__main__&quot;, line 6, in __main__.g\nFailed example:\n    g(10,  0.0)\nExpected:\n    10\nGot:\n    10.0\n**********************************************************************\n1 items had failures:\n   1 of   2 in __main__.g\n***Test Failed*** 1 failures.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TestResults(failed=1, attempted=4)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}